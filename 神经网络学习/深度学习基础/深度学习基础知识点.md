1. 基本原理，基于多元函数的链式求导法则，使用一阶优化算法，进行模型的训练。

2. 两种基本的求导方法：

   1. 前向求导：每个张量同时保存自己对所有变量的导数（n×m，m通常很大）。
      1. 计算方便（但不一定计算量小），每个运算可以立即得出结果的导数项，不需要保存中间变量。
      2. 占用内存多。
   2. 反向求导：每个张量同时保存**某个**变量对自己的导数（p×n，p通常为1）
      1. 计算繁琐（但不一定计算量大），每个运算需要不能直接得出导数项，需要保存中间变量。
      2. 占用内存少。
   3. 在深度中，通常采用反向求导法。

3. 优化算法：

   1. 基于梯度的优化算法：即变量更新步长仅和当前导数值相关。（容易停在鞍点）
   2. 基于梯度+动量的优化算法：模拟小球从山上滚下，即变量更新步长不仅和当前导数值相关，还与历史的步长相关。（可以摆脱鞍点束缚）
   3. 随机梯度下降（SGD）：真正的优化目标是使得模型在整个数据集上的Loss值最小，SGD每次计算梯度仅采用了数据集上一小部分数据，所以梯度方向不一定完全准确。但由于有一定的随机性，所以也可以帮助摆脱鞍点束缚。
   4. 二阶梯度算法：空间复杂度太大o(n²)，且需要解线性方程，计算量大。在深度学习上没有被采用。

4. 计算图：

   1. 变量作为边，算子作为节点的有向无环图。
   2. 每进行一次计算，就是在向计算图里添加节点和边。
   3. 逆着边的方向，就是反向求导的过程。

5. 张量维度的含义（`pytorch`）`[a, b, ...]`

   1. 最高维为batch，**通常**对于不同batch会进行相同的运算。
   2. 第二维为feature，指数据的特征，特征可以具有不同的长度。这一维上一般采用线性组合的方式进行运算（即矩阵乘法）。
   3. 其余维为数据本身的维度，如图像数据有两维宽高，音频数据则有一维时间。这些维度上的运算方式比较多样，对于二维图像数据，通常采用卷积的方式。

6. 梯度消失与梯度爆炸：链式法则导致的梯度数值过小或过大。如`sigmoid`函数导数最大为`0.25`。

7. 常用的算子（也叫层），每个层包括若干个运算和参数：

   1. `nn.Linear`：矩阵乘法`y=xW+B`。输入二维张量

   2. `nn.Conv2d`：二维卷积运算。输入四维张量。

   3. `nn.ConvTranspose2d`：二维反卷积运算。输入四维张量。

   4. `nn.MaxPool2d`：二维最大池化。输入四维张量。

   5. `nn.BatchNorm2d`：二维批归一化。

   6. 激活函数：

      1. `nn.ReLU`：
         $$
         y = 
         \begin{equation}
         \left\{
         	\begin{array}{lr}
         	0, & x \lt 0 \\
         	x, & x \ge 0
         	\end{array}
         \right.
         \end{equation}
         $$
         
      2. `nn.LeakyReLU`：
         $$
         y = 
         \begin{equation}
         \left\{
         	\begin{array}{lr}
         	\alpha x, & x \lt 0 \\
         	x, & x \ge 0
         	\end{array}
         \right.
         \end{equation}
         $$
         
      3. `nn.ReLU6`：
         $$
         y = 
         \begin{equation}
         \left\{
         	\begin{array}{lr}
         	0, & x \lt 0 \\
         	x, & 0 \le x \le 6 \\
         	6, & x \gt 6
         	\end{array}
         \right.
         \end{equation}
         $$
         
      4. `nn.SiLU`
         $$
         y = x*sigmoid(x)
         $$
      
      5. `nn.Sigmoid`
         $$
         y = \frac{1}{1+e^{-x}}
         $$
         
      6. `nn.Softmax`
         $$
         y_i = \frac{e^{x_j}}{\sum_{i} e^{x_i}}
         $$
         
      7. `nn.Tanh`
         $$
         y = \frac{e^x-e^{-x}}{e^x+e^{-x}}
         $$

